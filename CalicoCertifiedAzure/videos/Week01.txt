Week01
31/07/2022


#1.
Get Started With Azure Networking [Video]

Azure
vast network of physical Datacenters

Azure Global Infrastructure
massive network of physical Datacenters

Datacenter
where the cloud lives we connect to

Azure Datacenters
physical buildings in the Azure Global Infrastructure

Azure region
group of Datacenters using a regional low latency network connection

Azure geography
one or more of these Azure regions

Azure availability zones
refer to physical locations in an Azure region that are tolerant
to local failures

each availability zones has independent power that is connected
to the Azure Global Infrastructure

Minimum 3x separate availability zones in all enabled Azure regions


VNET
Azure V[irtual] NETwork utilizes [network] address space and subnets
to provide logically isolated networking space for your resources

Resources assigned to a VNET will consume IP addresses that are available
in the VNET subnet pool to establish networking

All resources in same VNET can communicate w/ each other
unless filters or subnet limitations applied


#2.
Kubernetes Cluster Options in Azure [Video]
AKS	Managed
BYOC	Self-Managed	Bring Your Own Cloud

AKS
run workloads w/o maintaining the control plane

BYOC
user relies on application that automates the deployment +
helps w/ maintenance of the cluster


AKS
control plane = FREE
fully managed by Azure = high-availability, redundancy
integrate w/ Azure services like 
ACI	Azure Container Instances
ACR	Azure Container Registry
AD	Azure Active Directory

AKS has officially built-in support for Calico
but
cannot ssh into control plane nor see in cluster node list


BYOC
Self-Managed
flexibility but limited 


#3.
Kubernetes Networking Options in Azure [Video]

Azure offers two networking models w/ AKS cluster solution:

Kubenet		networking models
Azure CNI	networking models



Azure CNI
Azure VNET
Address space
10.0.0.0/16
specify azure for advanced networking configuration

Kubenet
default


VNET address range : 10.0.0.0/16
Subnet pool : 10.0.0.0/24
Cluster cidr : 192.168.0.0/24




Azure CNI
open source project
both node and pod IPs are taken from the Azure VNET Subnet pool

permit Azure resources to directly connect to the internal
resources within the cluster and allows advanced features
e.g.
virtual nodes
Azure network policies to be used in that cluster

IP address exhaustion is a huge drawback associated with Azure CNI
e.g.
small cluster runs out of IP address and need bigger subnet pool
and migrate cluster


Kubenet
default option for creating a managed cluster in Azure
simple network plugin that offers basic networking features to cluster
run along other CNI plugins to implement features like policy enforcement

Each node receives an IP address from the Azure virtual network subnet
Pods receive IP addresses that are internal to the cluster from the
local host IPAM plugin

Your policy engine
az aks create --network-plugin azure   --network-policy calico
az aks create --network-plugin kubenet --network-policy calico


Calico is officially baked into the Azure Managed Services
i.e. for network policy enforcement

use Calico security policy engine by adding argument to AKS deployment
--network-policy calico


#4.
Kubernetes Network Security in Azure [Video]
Resource

Network Security Group
w/o NSG associated w/ your network capable resources, no one can access
them from the outside

Network Security Rule
each NSG groups together many NSRs
way to classify network traffic

NSR
Name
Priority
Source / Dest
Protocol
Direction
Port Range
Action

Not recommended to use NSGs to secure Kubernetes cluster
instead
use a policy engine to enforce security w/in your cluster borders

Kubernetes has Network Policy resource but does not enforce these 
network policies by default - it delegates the enforcement to CNIs


Network Policy can be applied to Pods, IP CIDRs, protocols, ports
directions

Calico security policy applied to network cards, containers, service
accounts side by side w/ Kubernetes network policy


Azure policy addon
can be enabled on-demand in AKS clusters
can only be deployed to Linux node pools
can only be enabled in clusters equipped w/ Azure CNI
i.e.
locked to one Vendor = Microsoft


#5.
Exposing Services Outside the Cluster [Video]
remember pods are ephemeral!

Kubernetes services 
abstract way to expose Pods by using a single DNS entry

NodePort	accessed externally
publish a node port on the Internet you need to use Network Address Translation i.e. NAT

LoadBalancer
service gets public IP address from the cloud providers infrastructure
allowing requests to be offloaded directly to the cloud providers infrastructure and then sent to the cluster

great for scale as the cloud provider can handle massive number of
requests but LoadBalancer is unaware of cluster size = can send
traffic unequally to pods which might render cluster unresponsive


Ingress [Kubernetes service]
elegant way to expose HTTP(S) routes to cluster services

create an Ingress service in your cluster after installing an Ingress
controller

Ingress controller can receive traffic and figure our the user's desired destination by examining URL path or prefix => re-routing traffic to the
responsible service(s) inside the cluster

Azure classic Load Balancers have two types both of which operate in Layer 4 of the OSI model
1. Basic
2. Standard

Azure also has AGIC
Application Gateway Ingress Controller
that can be used to avoid multiple hops

AGIC talks directly to pods by using their private IP address and
doesn't require KubeProxy services to establish communication


HTTP traffic routing
can find in AKS cluster networking tab in the Azure portal
but currently not suitable for Production environment


Client source IP address preservation

when traffic hits your cluster the client source IP address changes
Why?
because kube-proxy pods that are in charge of moving the traffic use
Network Address Translation i.e. NAT to move it

but
Azure AGIC provides workaround that can attach the client IP address
to the request as an HTTP forwarder-for header but you have to give up
features like TLS pass-through

big deal since you don't have the external client IP address to
associate with things like logs, debugging, or security concerns
...
eBPF dataplane
elegant solution for the source IP problem + more throughput