Weekk04
03/07/2021
 
https://courses.academy.tigera.io/courses/course-v1:tigera+CCO-L1+CCO-L1-2020/courseware/9472c002e19f4aeebed95cbb4414c88d/8a819660f9a34ba29b4173e9d2a9a871/?child=first


Kubernetest Services
way to abstract access to group of pods as network service


when client connects to service the connection is load balanced to one of the
pods backing the service


ClusterIP
when pod tries to connect to ClusterIP the connection is intercepted by rules
kube-proxy has been programmed into the kernel which select a random backing
pod to load balance to

changing the dest IP to be IP of backing pod using DNAT, the Linux kernel
tracks the state of these connections and auto reverses DNAT for return packets


NodePort
similar process however source IP address is changed as well as dest IP addr
from client pod IP to the node IP

if kube-proxy didn't do this then return packets leaving the backing pod node
would go directly to the client w/o giving node that did the NAT a chance to
reverse the NAT
i.e.
client would drop traffic because it wouldn't recognize it as being part of
the connection it made to the NodePort

Exception
service configured w/ ExternalTrafficPolicy local
kube-proxy only load balances to backing pods on the same node
i.e.
DNAT preserving client source IP address


LoadBalancer
repeat the same concepts
LB
typically located at point in the network where return traffic is guaranteed to routed via it
so
only has to do DNAT for its load balancing 

it load balances the traffic across the node using the corresponding NodePort
of the service

kube-proxy then follows the same processes as it did for the standard port
NAT both source + dest IP addresses

the return then follow the same path back to the client


some load balances still support ExternalTrafficPolicy local
in this case they will only load balance to nodes hosting a backing pod

and kube-proxy will only load balance to backing pods on the same node
preserving the org source IP addr all the way to the backing pod


Introduction to Kube-Proxy
explore load balancing rules kube-proxy programs into the kernel

by default
kube-proxy uses Linux iptables for these rules

or
can be configured to use Linux IPVS dataplanes	perf benefits

or
Calico eBPF native service handling
which doesn't use kube-proxy at all	out performs kube-proxy


kube-proxy iptables


Examine Kubernetes Services
kubectl get svc -n yaobank

kubectl get endpoints -n yaobank
NAME       ENDPOINTS                          AGE
database   198.19.21.66:2379                  3d15h
summary    198.19.21.67:80,198.19.22.130:80   3d15h
customer   198.19.22.129:80                   3d15h


kubectl get pods -n yaobank -o wide


Kube-Proxy Cluster IP Implementation
kube-proxy using IP tables			NOT IPVS mode

kube-proxy uses DNAT to map ClusterIP to the chosen backing pod


Pod A	Summary		node1
Pod B	Database	node2

commuincates using kube-proxy iptables


kubectl get endpoints -n yaobank summary
ubuntu@host1:~$ kubectl get endpoints -n yaobank summary
NAME      ENDPOINTS                          AGE
summary   198.19.21.67:80,198.19.22.130:80   3d17h



The summary service has two endpoints (198.19.21.1 on port 80, and 198.19.22.131 on port 80, in this example output)


ssh node1

Examine the KUBE-SERVICE chain

iptables chains
used to manage large numbers of iptables rules into groups

kube-proxy puts its top level rules into KUBE-SERVICES chain
e.g.

sudo iptables -v --numeric --table nat --list KUBE-SERVICES


Each iptables chain consists of list of rules executed in order until rule match

target		chain iptable will jump if rule match
prot		protocol match
srce + dest	source + dest IP address match
comments	kube-proxy includes
additional


ubuntu@node1:~$ sudo iptables -v --numeric --table nat --list KUBE-SERVICES | grep -E summary
    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !198.19.16.0/20       198.19.40.210        /* yaobank/summary:http cluster IP */ tcp dpt:80
    0     0 KUBE-SVC-OIQIZJVJK6E34BR4  tcp  --  *      *       0.0.0.0/0            198.19.40.210        /* yaobank/summary:http cluster IP */ tcp dpt:80


KUBE-SEP-XXXXXXXXXXXXXXXX -> summary pod
sudo iptables -v --numeric --table nat --list KUBE-SVC-OIQIZJVJK6E34BR4


Recap
trace kube-proxy iptables rules used to load balance traffic to summary pods
exposed as a service of type ClusterIP


SEP
Service EndPoint


packet being sent to clusterIP

KUBE-SERVICES chain matches clusterIP and jumps to KUBE-SVC chain
KUBE-SVC chain load balances packet to random service endpoint KUBE-SEP chain
KUBE-SEP chain DNAT packet so it gets routed to service endpoint backing pod



Kube-Proxy NodePort Implementation

iptables rules kube-proxy programs into the kernel to implement Node Port
based services
e.g.
Customer service

External clients use Customer service to connect to Customer pods

kube-proxy uses NAT to map Node Port to chosen backing pod and source IP to the
node IP of the ingress node so it can reverse the NAT for return packets


kubectl get endpoints -n yaobank customer

ubuntu@host1:~$ kubectl get endpoints -n yaobank customer
NAME       ENDPOINTS          AGE
customer   198.19.22.129:80   3d17h


198.19.22.129
starting from KUBE-SERVICES iptables chain, traverse each chain to get the rule
directing traffic to this endpoint IP address

KUBE-SERVICES -> KUBE-NODEPORTS

sudo iptables -v --numeric --table nat --list KUBE-SERVICES | grep KUBE-NODEPORTS
ubuntu@node1:~$ sudo iptables -v --numeric --table nat --list KUBE-SERVICES | grep KUBE-NODEPORTS
  153  9180 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL


ADDRTYPE match dst-type LOCAL
matches any packet w/ a local host IP as the destination
i.e.
any address that is assigned to one of the host interfaces


KUBE-NODEPORTS -> KUBE-SVC-XXXXXXXXXXXXXXXX
sudo iptables -v --numeric --table nat --list KUBE-NODEPORTS


    0     0 KUBE-SVC-PX5FENG4GZJTCELT  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* yaobank/customer:http */ tcp dpt:30180

rule directs traffic destined for customer service to the chain that load
balances the service


KUBE-SVC-XXXXXXXXXXXXXXXX -> KUBE-SEP-XXXXXXXXXXXXXXXX

sudo iptables -v --numeric --table nat --list KUBE-SVC-PX5FENG4GZJTCELT

ubuntu@node1:~$ sudo iptables -v --numeric --table nat --list KUBE-SVC-PX5FENG4GZJTCELT
Chain KUBE-SVC-PX5FENG4GZJTCELT (2 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 KUBE-SEP-5ACZIZ6RUWUYDTM3  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* yaobank/customer:http */

only single backing pod for customer service
no load balancing so there is single rule that directs all traffic to the chain
that delivers the packet to the service endpoint

sudo iptables -v --numeric --table nat --list KUBE-SEP-5ACZIZ6RUWUYDTM3

ubuntu@node1:~$ sudo iptables -v --numeric --table nat --list KUBE-SEP-5ACZIZ6RUWUYDTM3
Chain KUBE-SEP-5ACZIZ6RUWUYDTM3 (1 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 KUBE-MARK-MASQ  all  --  *      *       198.19.22.129        0.0.0.0/0            /* yaobank/customer:http */
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* yaobank/customer:http */ tcp to:198.19.22.129:80


Recap
trace kube-proxy iptables rules to load balance traffic to customer pods
exposed as service type NodePort

KUBE-SERVICES chain jumps to the KUBE-NODEPORTS chain

KUBE-NODEPORTS chain matches NodePort and jumps to KUBE-SVC chain
KUBE-SVC chain load balances packet to random service endpoint KUBE-SEP
KUBE-SEP chain DNAT packet so it will get routed to service ep backing pod


Calico Native Service Handling
https://courses.academy.tigera.io/courses/course-v1:tigera+CCO-L1+CCO-L1-2020/courseware/9472c002e19f4aeebed95cbb4414c88d/a8c8b42bfc554219a0a2dae2ae78674c/?child=first


eBPF
alternative to using kube-proxy

source IP preservation	=> easier network policy

DSR
Direct Server Return	=> improved performance
reduce the number of network hops for return traffic

load balance independent of network topology w/ reduced CPU + latency
compared to kube-proxy

rec'd incoming connection from external client Calico native service handling
is able to load balance the connection, fwd'ing the packets to another node if
req'd w/o NAT

rec'ing node performs DNAT to send traffic to backing pod
reverse packets get DNAT reversed and if DSR enabled then send directly back to client


About Calico eBPF
alternative to Linux data plane	based on iptables

scales higher throuhgput
has native support for Kubernetes services w/o needing kube-proxy
supports DSR

https://docs.projectcalico.org/maintenance/ebpf/enabling-bpf


NodePort without source IP preservation

kube-proxy
client source IP NOT preserved all the way to pod backing service

kube-proxy
uses NAT to map dest IP to chosen backing pod DNAT and map source IP to node IP
of the ingress node SNAT so standard Linux networking routes the return packets
to the ingress node so it can reverse the NAT


2x terminals
multipass shell host1

kubectl logs -n yaobank -l app=customer --follow

curl 198.19.0.1:30180


NOTE
if cluster configured to use overlay [VXLAN or IPIP] or WireGuard then SNAT
would make traffic appear to compe from teh IP address associated w/ corresponding
virutal interface on hte ingress node


Enable Calico eBPF

1. connect directly to API server [don't use kube-proxy]
2. disable kube-proxy
3. configure Calico to switch to eBPF dataplane


Configure Calico to connect directly to the API server
Calico supports a ConfigMap to configure direct connections for all components

note
talk to load balancer or domain name where necessary so it stil works

but for us single node hosting Kubernetes API service

cat <<EOF | kubectl apply -f -
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kubernetes-services-endpoint
  namespace: tigera-operator
data:
  KUBERNETES_SERVICE_HOST: "198.19.0.1"
  KUBERNETES_SERVICE_PORT: "6443"
EOF


Wait 1min and restart

kubectl delete pod -n tigera-operator -l k8s-app=tigera-operator

watch kubectl get pods -n calico-system


Disable kube-proxy

kube-proxy normally runs as a daemonset
remove kube-proxy from every node by add nodeSelector to that daemonset
which excludes all nodes

calicoctl patch felixconfiguration default --patch='{"spec": {"bpfKubeProxyIptablesCleanupEnabled": false}}'


Switch on eBPF mode

calicoctl patch felixconfiguration default --patch='{"spec": {"bpfEnabled": true}}'

kubectl delete pod -n yaobank -l app=customer
kubectl delete pod -n yaobank -l app=summary


Source IP preservation

after switch to handle packets w/ eBPF rather than standard Linux networking pipeline

allows to preserve source IP address including return traffic from original ingress
node

kubectl logs -n yaobank -l app=customer --follow

curl 198.19.0.1:30180



Direct Server Return (DSR)

allows node hosting service backing pod to send and return traffic directly
to the external client rather than take extra hop via ingress node
[control node in our example]


Snoop traffic without DSR
ssh control
sudo tcpdump -nvi any 'tcp port 30180'


other terminal
curl 198.19.0.1:30180

Traffic flowing via node port in both directions
e.g.
host1 			to node port	control node
198.19.0.1.30180 	> 198.19.0.1.30180 

node port		to host1
198.19.0.1.30180 	> 198.19.0.1.30180 


Switch on DSR
calicoctl patch felixconfiguration default --patch='{"spec": {"bpfExternalServiceMode": "DSR"}}'

now see traffic in only one direction 
e.g.
host1 			to node port	control node
198.19.0.1.30180 	> 198.19.0.1.30180 

return traffic going directly back to the client [host1] from the node hosting
customer pod backing the service	[node1]


Introduction to Advertising Services

alternative to node port or load balance is to advertize service IP addresses
over BGP
e.g.
on prem or private cloud deployments

use routers to do work => irrespective of topology + network is service aware


Advertise Cluster IP Range

ubuntu@host1:~$ ip route
default via 10.229.120.1 dev ens4 proto dhcp src 10.229.120.118 metric 100 
10.229.120.0/24 dev ens4 proto kernel scope link src 10.229.120.118 
10.229.120.1 dev ens4 proto dhcp scope link src 10.229.120.118 metric 100 
198.19.0.0/20 dev ens4 proto kernel scope link src 198.19.15.254 
198.19.28.208/29 via 198.19.0.2 dev ens4 proto bird 


proto bird
Calico learned to access nginx pod create externally


Advertize Kubernetes services rather than individual pods over BGP

cat <<EOF | calicoctl apply -f -
---
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  serviceClusterIPs:
  - cidr: "198.19.32.0/20"
EOF


calicoctl get bgpconfig default -o yaml

ubuntu@host1:~$ calicoctl get bgpconfig default -o yaml
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  creationTimestamp: "2021-07-04T11:29:30Z"
  name: default
  resourceVersion: "63940"
  uid: bfd76acc-4fb8-4ae9-8ff6-db09b3bef9f8
spec:
  serviceClusterIPs:
  - cidr: 198.19.32.0/20



NOW
ubuntu@host1:~$ ip route
default via 10.229.120.1 dev ens4 proto dhcp src 10.229.120.118 metric 100 
10.229.120.0/24 dev ens4 proto kernel scope link src 10.229.120.118 
10.229.120.1 dev ens4 proto dhcp scope link src 10.229.120.118 metric 100 
198.19.0.0/20 dev ens4 proto kernel scope link src 198.19.15.254 
198.19.28.208/29 via 198.19.0.2 dev ens4 proto bird 
198.19.32.0/20 proto bird 
	nexthop via 198.19.0.1 dev ens4 weight 1 
	nexthop via 198.19.0.2 dev ens4 weight 1 
	nexthop via 198.19.0.3 dev ens4 weight 1 


traffic to any service cluster IP address will get load balanced across all nodes
in the cluster by the network using ECMP

Equal Cost Multi Path

kube-proxy or Calico native service handling then load balances the cluster IP
across the service endpoints [backing pods] in exactly the same way as if a pod
had accessed a service via a cluster IP


kubectl get svc -n yaobank customer

curl 198.19.35.118


Advertizing cluster IP provides an alternative way to access services via Node Port
[simplfiy client service discovery w/o clients need to understand DNS SRV records]
OR
external network load balancers		less cost


Advertise External IPs

configure service to have one or more external IPs

kubectl get svc -n yaobank

ubuntu@host1:~$ kubectl get svc -n yaobank
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
database   ClusterIP   198.19.40.101   <none>        2379/TCP       4d15h
summary    ClusterIP   198.19.40.210   <none>        80/TCP         4d15h
customer   NodePort    198.19.36.165   <none>        80:30180/TCP   4d15h


NOTE
no external IP


Update BGP configuration
advertise a service external IP CIDR range of 198.19.48.0/20

calicoctl patch BGPConfig default --patch \
   '{"spec": {"serviceExternalIPs": [{"cidr": "198.19.48.0/20"}]}}'

ubuntu@host1:~$ ip route
default via 10.229.120.1 dev ens4 proto dhcp src 10.229.120.118 metric 100 
10.229.120.0/24 dev ens4 proto kernel scope link src 10.229.120.118 
10.229.120.1 dev ens4 proto dhcp scope link src 10.229.120.118 metric 100 
198.19.0.0/20 dev ens4 proto kernel scope link src 198.19.15.254 
198.19.28.208/29 via 198.19.0.2 dev ens4 proto bird 
198.19.32.0/20 proto bird 
	nexthop via 198.19.0.1 dev ens4 weight 1 
	nexthop via 198.19.0.2 dev ens4 weight 1 
	nexthop via 198.19.0.3 dev ens4 weight 1 
198.19.48.0/20 proto bird 
	nexthop via 198.19.0.1 dev ens4 weight 1 
	nexthop via 198.19.0.2 dev ens4 weight 1 
	nexthop via 198.19.0.3 dev ens4 weight 1 

Assign the service external IP
kubectl patch svc -n yaobank customer -p  '{"spec": {"externalIPs": ["198.19.48.10"]}}'

ubuntu@host1:~$ kubectl get svc -n yaobank
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
database   ClusterIP   198.19.40.101   <none>         2379/TCP       4d15h
summary    ClusterIP   198.19.40.210   <none>         80/TCP         4d15h
customer   NodePort    198.19.36.165   198.19.48.10   80:30180/TCP   4d15h


curl 198.19.48.10
still get 500 error


Review
5x different ways to connect pods from outside the cluster

1. via standard NodePort on specific node
2. direct to the pod IP address using externally routable IP Pools
3. advertize the service cluster IP range using ECMP to load balance across nodes
4. advertize individual cluster IP to load balance to nodes hosting pods backing svc
5. advertize service external IP to consume outside cluster IP range