Weekk01
29/06/2021


4x principles
01. every pod gets its own IP address
02. containers w/in pod share that iP address + can communicate freely w/ each other
03. pods can communicate w/ other pods in the cluster using their IP addresses w/o NAT	IP addresses are preserved across pods
04. network isolation that restricts what each pod can communicate w/ is defined using network policy

IP per pod		like VM
pod + container		like process


Network Policy isolation`	means simple "flat" network


Kubernetes does NOT have networking configured out-of-the-box
must use CNI plugins to configure networking
e.g.
Calico Network CNI plugin
Calico IPAM    CNI plugin


DNS
Example service			my-svc.my-namespace.svc.cluster-domain.example
Pod in cluster			my-svc.my-namespace
Pod in my-namespace		my-svc


Outgoing NAT
when pod tries to access outside the cluster the pod IP must be NAT'd to node IP
esp.
if pod network is an overlay network


LAB
Control (198.19.0.1) 		- Kubernetes control-plane node	10.255.255.1
Node1   (198.19.0.2) 		- Kubernetes worker node		10.255.255.2
Node2   (198.19.0.3) 		- Kubernetes worker node		10.255.255.3
Host1   (198.19.15.254) 	- General purpose host			10.255.255.254


Kubernetes cluster
Control		master
Node1			worker
Node2			worker

Host1			client		outside cluster


Local workstation
Linux with Snapcraft
installed on Ubuntu 20.04


Multiplass
https://multipass.run

sudo snap install multipass
multipass launch --name foo
multipass exec foo -- lsb_release -a

multipass launch -n bar --cloud-init cloud-config.yaml
// couldn't get working as couldn't find concrete yaml
https://cloudinit.readthedocs.io/en/latest/topics/examples.html

multipass stop foo
multipass delete foo
multipass purge

Restart
https://courses.academy.tigera.io/courses/course-v1:tigera+CCO-L1+CCO-L1-2020/courseware/34335c1507344450bbda775dd4460119/bf1560778a764aeaada7bb423860bcce/?child=first

See option to open Multipass top right
but the terminal is too small so instead launch std terminal
multipass shell


Create the lab environment

curl https://raw.githubusercontent.com/tigera/ccol1/main/control-init.yaml | multipass launch -n control -m 2048M 20.04 --cloud-init -
curl https://raw.githubusercontent.com/tigera/ccol1/main/node1-init.yaml | multipass launch -n node1 20.04 --cloud-init -
curl https://raw.githubusercontent.com/tigera/ccol1/main/node2-init.yaml | multipass launch -n node2 20.04 --cloud-init -
curl https://raw.githubusercontent.com/tigera/ccol1/main/host1-init.yaml | multipass launch -n host1 20.04 --cloud-init -


multipass shell host1
kubectl get nodes -A

NotReady
because we have not installed the CNI plugin to provide networking

host1
primary entry point

other instances acting as the cluster in the cloud


4x different ways to install Calico
Manifest
Operator
Managed Kubernetes Services
Kubernets Installers + Distros


Course uses operator based install


Install Calico
multipass shell host1

kubectl create -f https://docs.projectcalico.org/archive/v3.16/manifests/tigera-operator.yaml

kubectl get pods -n tigera-operator

cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    containerIPForwarding: Enabled
    ipPools:
    - cidr: 198.19.16.0/21
      natOutgoing: Enabled
      encapsulation: None
EOF


Validate installation
kubectl get tigerastatus/calico

kubectl get pods -A
kubectl get pods -n calico-system


Calico-node
runs on every K8s cluster node as a Daemonset
enforces network policy, setup routes on nodes, managing virtual interfaces
e.g. VXLAN or WireGuard


calico-typha
stateful proxy for K8s API server


calico-kube-controller
runs variety of Calico specific controllers that automate synch of resources
e.g.
when K8s node is deleted it tidies up any IP addresses or other
Calico resources associated with the node

kubectl get nodes -A


YaoBank
Yet AnOther Bank

Install sample app on host1

multipass shell host1
kubectl apply -f https://raw.githubusercontent.com/tigera/ccol1/main/yaobank.yaml

kubectl get all -n yaobank


Customer	GUI		Nodeport
Summary		BLL		Cluster IP
Database	DAL		Cluster IP


Deployment status
kubectl rollout status -n yaobank deployment/customer
kubectl rollout status -n yaobank deployment/summary
kubectl rollout status -n yaobank deployment/database


curl 198.19.0.1:30180


How did I know to use IP address?
198.19.0.1

It is at the top!

kubectl describe nodes | grep 198.19.0.1

this is from the control node

kubectl describe nodes control | grep 198.19.0.1
kubectl describe nodes control | more
kubectl describe nodes control | grep InternalIP


Manage the lab
on multipass shell
exit
multipass stop --all

multipass list

Name                    State             IPv4             Image
control                 Stopped           --               Ubuntu 20.04 LTS
host1                   Stopped           --               Ubuntu 20.04 LTS
node1                   Stopped           --               Ubuntu 20.04 LTS
node2                   Stopped           --               Ubuntu 20.04 LTS

multipass start control
multipass start node1
multipass start node2
multipass start host1


Teardown
multipass delete --all
multipass purge

 
