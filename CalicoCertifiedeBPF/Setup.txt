Setup
20/11/2021


Uninstall Multipass	if installed
Ubuntu Software Center	install Multipass

multipass list
multipass version


Week 1
Week 1	Lab Setup
#3.	Building and Validating the Lab Environment

multipass launch 21.04 --disk 10G -n ccol2ebpf
multipass shell ccol2ebpf


git clone https://github.com/libbpf/libbpf.git
git clone https://github.com/libbpf/libbpf-bootstrap.git

LIBBPF
cd libbpf/src
make


LIBBPF	BOOTSTRAP
cd ../../libbpf-bootstrap/
git submodule update --init --recursive



Week 2	Installing the Tools for GCP and Kubernetes on Your VM

multipass launch 21.04 --disk 10G -n ccol2ebpf
multipass shell ccol2ebpf

sudo apt update
sudo apt install -y python-is-python3

ssh-keygen -f ~/.ssh/google_compute_engine -N "" -b 4096

KUBECTL
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

CALICOCTL
curl -o calicoctl -O -L  "https://github.com/projectcalico/calicoctl/releases/download/v3.20.0/calicoctl"
sudo install -o root -g root -m 0755 calicoctl /usr/local/bin/calicoctl


GOOGLE SDK
curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-356.0.0-linux-x86_64.tar.gz
gunzip google-cloud-sdk-356.0.0-linux-x86_64.tar.gz
tar xvf google-cloud-sdk-356.0.0-linux-x86_64.tar
./google-cloud-sdk/install.sh

exit
multipass shell ccol2ebpf


Week 2	Initialising and Authenticating to GCP
./google-cloud-sdk/bin/gcloud init

Launch browser
Login into Google
steven_boland@hotmail.com

Copy + paste URL into browser
Copy + paste verification code


gcloud auth login
gcloud config set project steveproebpf
gcloud config list


Weej 2	Deploy Calico w/ eBPF
#3	Setting up some Basic Networking and Virtual Machines
gcloud compute project-info add-metadata --metadata google-compute-default-region=us-central1,google-compute-default-zone=us-central1-a


Login as steven_boland@hotmail.com
Created: steveproebpf
Created: SteveProBilling

Enabling service [compute.googleapis.com] on project [730223911331]...
HAD to link SteveProBilling

Enabling service [compute.googleapis.com] on project [730223911331]...
Operation "operations/acf.p2-730223911331-8cf85218-1301-4f68-92e6-da2afa013f60" finished successfully.
Updated [https://www.googleapis.com/compute/v1/projects/steveproebpf].



Created [https://www.googleapis.com/compute/v1/projects/steveproebpf/global/networks/ccol2ebpf-vpc].
NAME           SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
ccol2ebpf-vpc  CUSTOM       REGIONAL


gcloud compute networks subnets create ccol2ebpf-subnet \
  --network ccol2ebpf-vpc \
  --range 10.240.0.0/24

region
us-central1


Created [https://www.googleapis.com/compute/v1/projects/steveproebpf/regions/us-central1/subnetworks/ccol2ebpf-subnet].
NAME              REGION       NETWORK        RANGE          STACK_TYPE  IPV6_ACCESS_TYPE  IPV6_CIDR_RANGE  EXTERNAL_IPV6_CIDR_RANGE
ccol2ebpf-subnet  us-central1  ccol2ebpf-vpc  10.240.0.0/24  IPV4_ONLY


gcloud compute firewall-rules create ccol2ebpf-vpc-allow-internal \
  --allow tcp,udp,icmp \
  --network ccol2ebpf-vpc \
  --source-ranges 10.240.0.0/24


Creating firewall...\u2839Created [https://www.googleapis.com/compute/v1/projects/steveproebpf/global/firewalls/ccol2ebpf-vpc-allow-internal].
Creating firewall...done.                                                                                
NAME                          NETWORK        DIRECTION  PRIORITY  ALLOW         DENY  DISABLED
ccol2ebpf-vpc-allow-internal  ccol2ebpf-vpc  INGRESS    1000      tcp,udp,icmp        False


MY_PUBLIC_IP_AND_MASK=`curl -s https://checkip.amazonaws.com`/32
87.198.108.60/32



gcloud compute firewall-rules create ccol2ebpf-vpc-allow-external \
  --allow tcp:22,tcp:6443,icmp \
  --network ccol2ebpf-vpc \
  --source-ranges 87.198.108.60/32


Creating firewall...\u2839Created [https://www.googleapis.com/compute/v1/projects/steveproebpf/global/firewalls/ccol2ebpf-vpc-allow-external].
Creating firewall...done.                                                                                
NAME                          NETWORK        DIRECTION  PRIORITY  ALLOW                 DENY  DISABLED
ccol2ebpf-vpc-allow-external  ccol2ebpf-vpc  INGRESS    1000      tcp:22,tcp:6443,icmp        False



gcloud compute instances create ccol2ebpf-controller \
    --async \
    --boot-disk-size 200GB \
    --can-ip-forward \
    --image ubuntu-2004-focal-v20210908 \
    --image-project ubuntu-os-cloud \
    --machine-type n1-standard-2 \
    --private-network-ip 10.240.0.11 \
    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
    --subnet ccol2ebpf-subnet \
    --zone us-central1-a \
    --tags ccol2ebpf,controller

NOTE: The users will be charged for public IPs when VMs are created.
Instance creation in progress for [ccol2ebpf-controller]: https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a/operations/operation-1637417979566-5d13915055ae9-6607b430-a1bc89bc
Use [gcloud compute operations describe URI] command to check the status of the operation(s).


87.198.108.60/32
https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a/operations/operation-1637417979566-5d13915055ae9-6607b430-a1bc89bc


gcloud compute operations describe https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a/operations/operation-1637417979566-5d13915055ae9-6607b430-a1bc89bc


id: '2110875892597439763'
insertTime: '2021-11-20T06:19:41.078-08:00'
kind: compute#operation
name: operation-1637417979566-5d13915055ae9-6607b430-a1bc89bc
operationType: insert
progress: 100
selfLink: https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a/operations/operation-1637417979566-5d13915055ae9-6607b430-a1bc89bc
startTime: '2021-11-20T06:19:41.710-08:00'
status: DONE
targetId: '552914696822635795'
targetLink: https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a/instances/ccol2ebpf-controller
user: steven_boland@hotmail.com
warnings:
- code: DISK_SIZE_LARGER_THAN_IMAGE_SIZE
  data:
  - key: disk_size_gb
    value: '200'
  - key: image_size_gb
    value: '10'
  message: "Disk size: '200 GB' is larger than image size: '10 GB'. You might need\
    \ to resize the root repartition manually if the operating system does not support\
    \ automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd\
    \ for details."
zone: https://www.googleapis.com/compute/v1/projects/steveproebpf/zones/us-central1-a



Create 3x VMs
for i in 0 1 2; do
  gcloud compute instances create ccol2ebpf-worker-${i} \
    --async \
    --boot-disk-size 200GB \
    --can-ip-forward \
    --image ubuntu-2004-focal-v20210908 \
    --image-project ubuntu-os-cloud \
    --machine-type n1-standard-2 \
    --private-network-ip 10.240.0.2${i} \
    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
    --subnet ccol2ebpf-subnet \
    --zone us-central1-a \
    --tags ccol2ebpf,worker
done


gcloud compute instances list --filter="name~ccol2ebpf"


Week 2	Deploy Calico w/ eBPF
#4	Using Kubeadm To Bootstrap a Cluster

for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do echo $i; done

for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh ubuntu@$i --zone us-central1-a --command "echo \"Hello\""; done


ubuntu@ccol2ebpf:~$ gcloud compute instances list
NAME                  ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
ccol2ebpf-controller  us-central1-a  n1-standard-2               10.240.0.11  34.135.81.125   RUNNING
ccol2ebpf-worker-0    us-central1-a  n1-standard-2               10.240.0.20  35.222.255.58   RUNNING
ccol2ebpf-worker-1    us-central1-a  n1-standard-2               10.240.0.21  35.202.145.246  RUNNING
ccol2ebpf-worker-2    us-central1-a  n1-standard-2               10.240.0.22  35.224.107.66   RUNNING



Waiting for SSH key to propagate.
Warning: Permanently added 'compute.552914696822635795' (ECDSA) to the list of known hosts.
Hello
Warning: Permanently added 'compute.587031619214555756' (ECDSA) to the list of known hosts.
Hello
Warning: Permanently added 'compute.8732714001210063465' (ECDSA) to the list of known hosts.
Hello
Warning: Permanently added 'compute.8309883322846460519' (ECDSA) to the list of known hosts.
Hello

SSH
gcloud compute ssh ubuntu@ccol2ebpf-controller
gcloud compute ssh ubuntu@ccol2ebpf-worker-0
gcloud compute ssh ubuntu@ccol2ebpf-worker-1
gcloud compute ssh ubuntu@ccol2ebpf-worker-2


kubeadm pre-reqs
/etc/modules-load.d/k8s.conf

for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "echo \"br_netfilter\" | sudo tee /etc/modules-load.d/k8s.conf" ; done


ip tables see bridged traffic
/etc/sysctl.d/k8s.conf

for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "echo -e \"net.bridge.bridge-nf-call-ip6tables = 1\\nnet.bridge.bridge-nf-call-iptables = 1\" | sudo tee /etc/sysctl.d/k8s.conf" ; done


reboot
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo reboot" ; done


re-running
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "uptime" ; done


Install Docker
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo apt update && sudo apt install -y docker.io && sudo systemctl enable docker.service"; done


cgroup
feature of the Linux kernel that allows process and resource isolation


for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "echo -e '{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\"\n}' | sudo tee /etc/docker/daemon.json"; done


restart Docker
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo systemctl daemon-reload && sudo systemctl restart docker"; done


Kubernetes and kubeadm KEY
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo apt update && sudo apt install -y apt-transport-https ca-certificates curl"; done


download Google package installation key
for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg"; done


configure each VM to use the key 
/etc/apt/sources.list.d/kubernetes.list:


Install kubelet		Kubernetes node agent
Install kubeadm
Install kubectl

for i in `gcloud compute instances list --filter="name~ccol2ebpf" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo apt-get update && sudo apt-get install -y kubelet=1.21.4-00 kubeadm=1.21.4-00 kubectl=1.21.4-00 && sudo apt-mark hold kubelet kubeadm kubectl"; done


CONNECT
gcloud compute ssh ubuntu@ccol2ebpf-controller --zone us-central1-a


create file
cat << EOF > ccol2ebpf-kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
kubernetesVersion: v1.21.4
networking:
  podSubnet: "192.168.0.0/16"
apiServer:
  certSANs:
    - 88.88.88.88
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
EOF


get public IP and replace YAML file
MY_PUBLIC_IP=`curl -s https://checkip.amazonaws.com` && sed -i "s/88.88.88.88/${MY_PUBLIC_IP}/g" ccol2ebpf-kubeadm-config.yaml
34.135.81.125


build cluster controller node
sudo kubeadm init --config ccol2ebpf-kubeadm-config.yaml


export KUBECONFIG=/etc/kubernetes/admin.conf


Copy the two lines beginning "kubeadm join"
kubeadm join 10.240.0.11:6443 --token 7htdyq.54i4neodpl63r18c \
	--discovery-token-ca-cert-hash sha256:889c8851fd130ceded5c069754dbecd86c30c81e2cba2df253d803e232c9318e 


kubeconfig
mkdir -p $HOME/.kube && sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && sudo chown $(id -u):$(id -g) $HOME/.kube/config && cp $HOME/.kube/config $HOME/.kube/config.external_ip


substitute public IP in 2x kubconfig files
MY_PUBLIC_IP=`curl -s https://checkip.amazonaws.com` && sed -i "s/10.240.0.11/${MY_PUBLIC_IP}/g" $HOME/.kube/config.external_ip && diff .kube/config .kube/config.external_ip


exit
from ubuntu@ccol2ebpf-controlle


Multipass ccol2ebpf
mv ~/.kube ~/.kube.epochtime.$(date +%s).backup


copy config $HOME/.kube/config
mkdir /home/ubuntu/.kube && gcloud compute scp --zone us-central1-a ubuntu@ccol2ebpf-controller:/home/ubuntu/.kube/config.external_ip /home/ubuntu/.kube/config


kubectl get nodes
NAME                   STATUS     ROLES                  AGE    VERSION
ccol2ebpf-controller   NotReady   control-plane,master   5m4s   v1.21.4


join worker nodes

use
kubeadm join
but sub tokens
e.g.

for i in `gcloud compute instances list --filter="name~ccol2ebpf-worker" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command "sudo kubeadm join 10.240.0.11:6443 --token ---REDACTED--- --discovery-token-ca-cert-hash sha256:---REDACTED---" ; done

+

kubeadm join 10.240.0.11:6443 --token 7htdyq.54i4neodpl63r18c --discovery-token-ca-cert-hash sha256:889c8851fd130ceded5c069754dbecd86c30c81e2cba2df253d803e232c9318e

=


for i in `gcloud compute instances list --filter="name~ccol2ebpf-worker" | grep -v NAME | awk '{print $1}'`; do gcloud compute ssh --zone us-central1-a ubuntu@$i --command  "sudo kubeadm join 10.240.0.11:6443 --token 7htdyq.54i4neodpl63r18c --discovery-token-ca-cert-hash sha256:889c8851fd130ceded5c069754dbecd86c30c81e2cba2df253d803e232c9318e" ; done


kubectl get nodes
NAME                   STATUS     ROLES                  AGE     VERSION
ccol2ebpf-controller   NotReady   control-plane,master   8m38s   v1.21.4
ccol2ebpf-worker-0     NotReady   <none>                 34s     v1.21.4
ccol2ebpf-worker-1     NotReady   <none>                 23s     v1.21.4
ccol2ebpf-worker-2     NotReady   <none>                 13s     v1.21.4


built Healthy cluster
but has not networking	i.e. no CNI
which explains status "NotReady"

kubectl get pods -A
coredns pods are Pending waiting for node with networking to deploy on to


Week 2	Deploy Calico
#5	Installing Calico on the Cluster

Tigera operator
kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

Calico CRDs
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml

now coredns pods are running
i.e. CNI configured via Calico


Week 2	Deploy Calico
#6	Testing Performance with Calico’s Standard Iptables Data Plane

cluster running standard Linux IP tables data plane


k8s-bench-suite
git clone https://github.com/InfraBuilder/k8s-bench-suite.git

TEST
k8s-bench-suite/knb -v -cn ccol2ebpf-worker-0 -sn ccol2ebpf-worker-1


Week 2	Deploy Calico
#7	Switching to eBPF

kube api server
kubectl get configmap -n kube-system kube-proxy -o yaml | grep server
        server: https://10.240.0.11:6443


create CRD

cat << EOF > ccol2ebpf-k8s-endpoint.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: kubernetes-services-endpoint
  namespace: tigera-operator
data:
  KUBERNETES_SERVICE_HOST: "10.240.0.11"
  KUBERNETES_SERVICE_PORT: "6443"
EOF

kubectl apply -f ccol2ebpf-k8s-endpoint.yaml


Wait 60s
pickup change
sleep 60 && kubectl delete pod -n tigera-operator -l k8s-app=tigera-operator

disable kube-proxy
kube proxy is the per-node network proxy that implements Kubernetes services


kubectl get pods -n=kube-system


kubectl patch ds -n kube-system kube-proxy -p '{"spec":{"template":{"spec":{"nodeSelector":{"non-calico": "true"}}}}}'

kubectl get pods -n=calico-system

switch to eBPF
kubectl patch installation.operator.tigera.io default --type merge -p '{"spec":{"calicoNetwork":{"linuxDataplane":"BPF", "hostPorts":null}}}'

kubectl get pods -n=calico-system
At this point, your Kubernetes cluster should be healthy and running the Calico eBPF data plane.


#8	Testing Performance with Calico’s eBPF Data Plane
k8s-bench-suite/knb -v -cn ccol2ebpf-worker0 -sn ccol2ebpf-worker1


Week 2	Intro to Online Boutique
#1	Deploying Online Boutique

git clone https://github.com/GoogleCloudPlatform/microservices-demo.git


kubectl apply -f microservices-demo/release/kubernetes-manifests.yaml

kubectl get pods -o wide


kubectl get services | grep frontend


ip addr | grep global
    inet 10.80.26.77/24 brd 10.80.26.255 scope global ens3


10.80.26.77
kubectl port-forward service/frontend 8381:80 --address 10.80.26.77


Launch browser
http://10.80.26.77:8381

exit


Tear Down Your Cluster
Delete the VMs:
Delete the external firewall rules:
Delete the internal firewall rules:
Delete the subnet:
Delete the network:

gcloud compute instances delete --zone us-central1-a ccol2ebpf-controller ccol2ebpf-worker-0 ccol2ebpf-worker-1 ccol2ebpf-worker-2
gcloud compute firewall-rules delete ccol2ebpf-vpc-allow-external
gcloud compute firewall-rules delete ccol2ebpf-vpc-allow-internal
gcloud compute networks subnets delete ccol2ebpf-subnet
gcloud compute networks delete ccol2ebpf-vpc