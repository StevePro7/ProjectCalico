Calico Routing Modes
31/07/2022

https://www.youtube.com/watch?v=MpbIZ1SmEkU

Reference
https://joshrosso.com/docs/2020/2020-10-01-calico-routing-modes


IP tunnel
https://en.wikipedia.org/wiki/IP_tunnel
IP network communications channel between two networks


IP in IP
https://en.wikipedia.org/wiki/IP_in_IP
An IP tunneling protocol that encapsulates one IP packet into another

Virtual Extensible LAN
VXLAN
https://en.wikipedia.org/wiki/Virtual_Extensible_LAN
A network virtualization technology that attempts to address the scalability problems associated with large cloud computing deployments

VXLAN uses a VLAN like encapsulation technique to encapsulate layer 2 Ethernet frames within layer 4 UDP datagrams using 4789 destination UDP port number

VXLAN endpoints, which terminate VXLAN tunnels and may either be virtual or physical switch ports, are known as VXLAN Tunnel EndPoints [VTEPs]


ROUTE SHARING
Calico uses BGP to distribute routes amongst hosts
Calico node pods run on every host


VXLAN is great for networks that do not support IP-in-IP like Azure
or don't support BPG which is disabled in VXLAN mode


BGP
Border Gateway Protocol
https://en.wikipedia.org/wiki/Border_Gateway_Protocol
protocol designed to exchange routing and reachability information among autonomous systems on the the Internet


Route Sharing
Calico uses BGP Border Gateway Protocol to share routes

BGP protocol enables our nodes to peer together and communicate what routes they know about


kubectl get pods -n calico-system
kubectl get po -n calico-system calico-node-7cp4m -o yaml


    readinessProbe:
      exec:
        command:
        - /bin/calico-node
        - -bird-ready 
        - -felix-ready
        - -bgp-metrics-ready


Felix
bird


Calico uses BIRD to facilitate BGP route sharing
bird works in calico node instance and mesh / share routes btwn
all the different nodes

NB:
Calico does support more advanced configurations
e.g.
route reflectors
top of rack switches
that support communicating w/ BGP


Once the routes are shared then look at Felix

calicoctl
https://projectcalico.docs.tigera.io/maintenance/clis/calicoctl/install

ssh into master node and install calicoctl
e.g.
ssh -i master_ssh_key ubuntu@MASTER_NODE_PUBLIC_IP

ubuntu@ip-172-16-101-248:
curl -L https://github.com/projectcalico/calico/releases/download/v3.23.3/calicoctl-linux-amd64 -o calicoctl

chmod +x ./calicoctl

calicoctl version
sudo ./calicoctl node status

OR
sudo cp calicoctl /usr/local/bin/
sudo calicoctl node status


Listed are the 4x worker nodes calico is peered with 


FELIX
Felix is responsible for programming the hosts route table

ssh onto master
route -n

routing table

calico runs by default in IP-in-IP mode
thus you would see tunl0 interfaces
the way link the nodes from a routing perspective

the "cali" interfaces are pod IP addr that are on this node
the "cali" interfaces are leveraged for enforcing policy

packet routes in and goes to the interface thru IP-in-IP rules
OR IP-in-IP chains rules

BPG sharing routes + Felix is programming the hosts route table so
packets know where to go


Routing modes
IP-in-IP	mode	default	encapsulation
Direct		unencapsulated routing mode
VXLAN		encapsulation no BGP


ssh -i master_ssh_key ubuntu@WORKER_PUBLIC_IP


IP-in-IP
pod1 - pod2

Puts a packet in another packet
Outer Header + Inner Header

Outer Header 
srce	node
dest	node
proto	IPIP

Inner Header
srce	pod
dest	pod
proto	TCP


Direct 
unencapsulated traffic
highly performant


Calico
IP Pool
e.g.
sudo calicoctl --allow-version-mismatch get ippool
NAME                  CIDR             SELECTOR   
default-ipv4-ippool   192.168.0.0/16   all()  


sudo calicoctl --allow-version-mismatch get ippool -o yaml


spec:
  blockSize: 26
  cidr: 192.168.0.0/16
  ipipMode: Never
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: Never


All you need to do to change calico to use "direct" routing mode is to
change ipipMode from Always [default] to Never
ipipMode: Always
ipipMode: Never


All of the "tunl0" interfaces changes to "ens5"
i.e.
now no more tunneling => direct via eth0 type network interface
and the Gateway changes for different subnets


IMPORTANT
direct routing and source / destination checks

AWS Dashboard
EC2 Instances
click instance | Actions | Networking | Change srce/dest check


only one header now in "direct" routing mode
TCP protocol only
pod IPs


Ethernet II
Dest	worker 2 MAC address
Srce	worker 1 MAC address

worker 1
ip addr
see the corresponding MAC address


direct mode fails when routing across different subnets
but

IP-in-IP has 
CrossSubnet option available

when I am traversing a subnet boundary I'm going to encapsulate using
IP and IP but when I'm not i.e. communicating inside of the same subnet,
I am going to do the direct unencapsulating routing


update IPPool
spec:
  blockSize: 26
  cidr: 192.168.0.0/16
  ipipMode: CrossSubnet
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: Never


switched back to tunnel interface for different subnet route
i.e.
tunl0


VXLAN
encapsulation like technology
tunneling protocol
encapsulates layer2 ethernet frames into UDP packets
create virtual L2 networks


Why VXLAN
you can't use IP-in-IP 'cos your network won't allow BGP route sharing
OR
IP-in-IP doesn't work


Old days
calico	policy enforcement
flannel	routing


Download calico manifest YAML
IP-in-IP by default
BGP	default route share

#1.
change calico backend from bird to vxlan

ConfigMap
data:
  calico_backend: "bird"
  calico_backend: "vxlan"


#2.
IPPool in Felix config
- name: CALICO_IPV4POOL_IPIP
- name: CALICO_IPV4POOL_VXLAN


#3.
Disable bird liveness / readiness checks
remember bird is what facilitates BGP functionality
thus bird liveness / readiness checks would fail

livenessProbe:
  exec:
    command:
    - /bin/calico-node
    - -felix-live
    #- -bird-live


readinessProbe:
  exec:
    command:
    - /bin/calico-node
    - -felix-ready
    #- -bird-ready


ssh on to nodes
route -n

now there are 
vxlan.calico

Wireshark
Ethernet II
Internet Protocol			srce/dest hosts
User Datagram Protocol
Virtual eXtensible Local Area Network
Ethernet II				virtual layer2 packet inside

IMPORTANT
the 2nd Ethernet II has the MAC address that matches
ip addr | grep vxlan

operating on that extra virtual layer2 network so we're really
virtualizing that entire network and not just doing simple IP-in-IP
encapsulation w/ that extra outer IP header that we're routing things
around


SUMMARY
route sharing capabilities 
BGP feature set that gets used in IP-in-IP and direct where we have 
these bird daemons that can peer together, share routes but in VXLAN
we completely disable BGP altogether and do not use it

route btwn different zones and the implications going btwn subnets
depending on our routing methods